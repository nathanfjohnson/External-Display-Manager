{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathanfjohnson/External-Display-Manager/blob/master/Nat_GAN_2022_03_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8etcEx5to2Io",
        "tags": []
      },
      "source": [
        "# Pre-work"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Drive, import globals { form-width: \"200px\", display-mode: \"form\" }\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def mount_drive():\n",
        "  from IPython import display\n",
        "  import os, sys\n",
        "  from google.colab import drive\n",
        "\n",
        "  drive.mount(\"/content/gdrive\", force_remount=False)\n",
        "\n",
        "  if os.path.isdir(\"/content/gdrive\"):\n",
        "      print(\"mounted\")\n",
        "      working_dir = \"/content/gdrive/MyDrive/nat_gan\"\n",
        "  else:\n",
        "      print(\"not mounted\")\n",
        "      working_dir = \"content\"\n",
        "  \n",
        "  return working_dir\n",
        "\n",
        "def make_system_command(command):\n",
        "  import os\n",
        "  debug = False # @param {type:\"boolean\"}\n",
        "  if debug:\n",
        "    !{command}\n",
        "  else:\n",
        "    print(command)\n",
        "    os.system(command)\n",
        "\n",
        "working_dir = mount_drive()"
      ],
      "metadata": {
        "id": "whX1VAArSsC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <h1>Run Parameters</h1> { display-mode: \"form\" }\n",
        "\n",
        "def build_run_args():\n",
        "  \n",
        "  # @markdown Name of files and directory (no / or spaces)\n",
        "  run_name = 'refactor-1'  # @param {type:\"string\"}\n",
        "  continuous_vqgan = False # @param {type:\"boolean\"}\n",
        "  vqgan_1_runs =  0# @param {type:\"integer\"}\n",
        "  diff_1_runs =  1# @param {type:\"integer\"}\n",
        "  vqgan_2_runs =  0# @param {type:\"integer\"}\n",
        "  diff_2_runs =  0# @param {type:\"integer\"}\n",
        "  vqgan_3_runs =  0# @param {type:\"integer\"}\n",
        "  end_frame =   1000# @param {type:\"integer\"}\n",
        "  latest_image_location = \"\" # @param {type:\"string\"}\n",
        "  save_mid_steps = False # @param {type:\"boolean\"}\n",
        "  seed =   112291 # @param {type:\"number\"}\n",
        "  ramdom_seed = True # @param {type:\"boolean\"}\n",
        "\n",
        "  args = {\n",
        "      \"run_name\" : run_name,\n",
        "      \"continuous_vqgan\" : continuous_vqgan,\n",
        "      \"diff_1_runs\" : diff_1_runs,\n",
        "      \"diff_2_runs\" : diff_2_runs,\n",
        "      \"vqgan_1_runs\" : vqgan_1_runs,\n",
        "      \"vqgan_2_runs\" : vqgan_2_runs,\n",
        "      \"vqgan_3_runs\" : vqgan_3_runs,\n",
        "      \"ramdom_seed\" : ramdom_seed,\n",
        "      \"save_mid_steps\" : save_mid_steps,\n",
        "      \"seed\" : seed,\n",
        "      \"end_frame\" : end_frame,\n",
        "      \"latest_image_location\" : latest_image_location,\n",
        "    }\n",
        "  return args\n"
      ],
      "metadata": {
        "id": "P0VNk50nD1MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <h1>Prompting</h1> { display-mode: \"form\" }\n",
        "\n",
        "def build_prompt_args():\n",
        "  import json\n",
        "  text_prompts = \"[{\\\"frames\\\": [0,99999],\\\"prompt\\\": \\\"vibrant electric refactor digital painting\\\",\\\"values\\\": [1,1]}]\"  # @param {type:\"string\"}\n",
        "  modifier_postpend = \"\" # @param {type:\"string\"}\n",
        "  image_prompt = \"\" # @param {type:\"string\"}\n",
        "  initial_image = '/content/gdrive/MyDrive/nat_gan/assets/Used/IMG_0412.png'  # @param {type:\"string\"}\n",
        "  add_random_adjectives = False # @param {type:\"boolean\"}\n",
        "  add_random_noun = False # @param {type:\"boolean\"}\n",
        "  add_random_styles = False # @param {type:\"boolean\"}\n",
        "  random_image_directory = \"\" # @param {type:\"string\"}\n",
        "  use_video_frames = \"\" # @param {type:\"string\"}\n",
        "\n",
        "  prompt_args = {\n",
        "                  \"initial_image\" : initial_image,\n",
        "                  \"text_prompts\" : json.loads(text_prompts),\n",
        "                  \"add_random_adjectives\" : add_random_adjectives,\n",
        "                  \"add_random_noun\" : add_random_noun,\n",
        "                  \"add_random_styles\" : add_random_styles,\n",
        "                  \"modifier_postpend\" : modifier_postpend,\n",
        "                  \"image_prompt\" : image_prompt,\n",
        "                  \"random_image_directory\" : random_image_directory,\n",
        "                  \"use_video_frames\" : use_video_frames,\n",
        "                }\n",
        "  return prompt_args"
      ],
      "metadata": {
        "id": "WoK1xgM5Eco2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <h1>Image Manipulation</h1> { display-mode: \"form\" }\n",
        "\n",
        "def build_manipulste_args(output_path):\n",
        "    # @markdown <h2>Image Overlay</h2>\n",
        "  copy_overlay_image = \"\" # @param {type:\"string\"}\n",
        "  overlay_operator = \"normal\" #@param [\"normal\",\"bumpmap\",\"color_burn\",\"color_dodge\",\"darken\",\"darken_intensity\",\"hard_light\",\"lighten\",\"lighten_intensity\",\"linear_burn\",\"linear_dodge\",\"linear_light\",\"modulus_subtract\",\"multiply\",\"overlay\",\"pegtop_light\",\"pin_light\",\"screen\",\"soft_light\",\"vivid_light\"]\n",
        "\n",
        "  # @markdown <h2>Blur Mask</h2>\n",
        "  # @markdown https://legacy.imagemagick.org/Usage/compose/#light\n",
        "  blur_mask_filename = \"\" # @param {type:\"string\"}\n",
        "\n",
        "  # @markdown <h2>Image Effects</h2>\n",
        "  blur = \"0\" # @param {type:\"string\"}\n",
        "  flip = False  # @param {type:\"boolean\"}\n",
        "  flop = False  # @param {type:\"boolean\"}\n",
        "  shake = False  # @param {type:\"boolean\"}\n",
        "  kaleidoscope = True # @param {type:\"boolean\"}\n",
        "  scale_x = \"1\" # @param {type:\"string\"}\n",
        "  scale_y = \"1\" # @param {type:\"string\"}\n",
        "  angle = \"72\" # @param {type:\"string\"}\n",
        "  origin_x = \"0.5\" # @param {type:\"string\"}\n",
        "  origin_y = \"0.5\" # @param {type:\"string\"}\n",
        "  translate_x = \"0\" # @param {type:\"string\"}\n",
        "  # @markdown **positives moves it down a percentage of the height**\n",
        "  translate_y = \"0\" # @param {type:\"string\"}\n",
        "  swirl =  \"0\" # @param {type:\"string\"}\n",
        "  wobble_angle = \"0\" # @param {type:\"string\"}\n",
        "  wobble_distance = \"0\" # @param {type:\"string\"}\n",
        "\n",
        "  # @markdown <h2>Barrel Distort</h2>\n",
        "  # @markdown <p>barrel zoom: 1 is nothing. 0.9 is zoom in, 1.1 is zoom out (very sensitive)</p>\n",
        "  barrel_zoom =  \"1\" # @param {type:\"string\"}\n",
        "\n",
        "  # @markdown **barrel distort:** 0 is nothing. negative is edges out, positive edges in.\n",
        "  barrel_distort = \"0\" # @param {type:\"string\"}\n",
        "  barrel_origin_x = \"0.5\" # @param {type:\"string\"}\n",
        "  barrel_origin_y = \"0.5\" # @param {type:\"string\"}\n",
        "\n",
        "  # @markdown <h2>Fred Script</h2>\n",
        "  fred_script = \"\" # @param {type:\"string\"}\n",
        "\n",
        "  overlay_path = f\"{output_path}/overlay.png\"\n",
        "\n",
        "  if copy_overlay_image is not \"\":\n",
        "    try:\n",
        "      shutil.copyfile(overlay_image_filename, overlay_path)\n",
        "    except:\n",
        "        pass\n",
        "  elif not file_exists(overlay_path):\n",
        "    from wand.image import Image as WandImage\n",
        "    with WandImage(width=1000, height=1000) as img:\n",
        "      img.save(filename=overlay_path)\n",
        "\n",
        "  overlay_image_filename = overlay_path\n",
        "\n",
        "  manipulste_args = {\n",
        "      \"scale_x\" : scale_x,\n",
        "      \"scale_y\" : scale_y,\n",
        "      \"shake\" : shake,\n",
        "      \"wobble_angle\" : wobble_angle,\n",
        "      \"wobble_distance\" : wobble_distance,\n",
        "      \"translate_x\" : translate_x,\n",
        "      \"translate_y\" : translate_y,\n",
        "      \"angle\" : angle,\n",
        "      \"barrel_distort\" : barrel_distort,\n",
        "      \"barrel_origin_x\" : barrel_origin_x,\n",
        "      \"barrel_origin_y\" : barrel_origin_y,\n",
        "      \"barrel_zoom\" : barrel_zoom,\n",
        "      \"flip\" : flip,\n",
        "      \"flop\" : flop,\n",
        "      \"fred_script\" : fred_script,\n",
        "      \"swirl\" : swirl,\n",
        "      \"kaleidoscope\" : kaleidoscope,\n",
        "      \"blur\" : blur,\n",
        "      \"blur_mask_filename\" : blur_mask_filename,\n",
        "      \"origin_x\" : origin_x,\n",
        "      \"origin_y\" : origin_y,\n",
        "      \"overlay_image_filename\" : overlay_image_filename,\n",
        "      \"overlay_operator\" : overlay_operator,\n",
        "    }\n",
        "\n",
        "  return manipulste_args\n",
        "\n",
        "def manipulate_image(wandImage, args, i):\n",
        "    from wand.image import Image as WandImage\n",
        "\n",
        "    angle=parse_number_value(args[\"angle\"], i)\n",
        "    barrel_distort=parse_number_value(args[\"barrel_distort\"], i)\n",
        "    barrel_origin_x=parse_number_value(args[\"barrel_origin_x\"], i)\n",
        "    barrel_origin_y=parse_number_value(args[\"barrel_origin_y\"], i)\n",
        "    barrel_zoom=parse_number_value(args[\"barrel_zoom\"], i)\n",
        "    blur=parse_number_value(args[\"blur\"], i)\n",
        "    blur_mask_filename=args[\"blur_mask_filename\"]\n",
        "    flip=args[\"flip\"]\n",
        "    flop=args[\"flop\"]\n",
        "    fred_script=args[\"fred_script\"]\n",
        "    kaleidoscope=args[\"kaleidoscope\"]\n",
        "    origin_x=parse_number_value(args[\"origin_x\"], i)\n",
        "    origin_y=parse_number_value(args[\"origin_y\"], i)\n",
        "    overlay_operator=args[\"overlay_operator\"]\n",
        "    scale_x=parse_number_value(args[\"scale_x\"], i)\n",
        "    scale_y=parse_number_value(args[\"scale_y\"], i)\n",
        "    shake=args[\"shake\"]\n",
        "    translate_x=parse_number_value(args[\"translate_x\"], i)\n",
        "    translate_y=parse_number_value(args[\"translate_y\"], i)\n",
        "    overlay_image_filename=args[\"overlay_image_filename\"]\n",
        "    swirl=parse_number_value(args[\"swirl\"], i)\n",
        "    wobble_angle=parse_number_value(args[\"wobble_angle\"], i)\n",
        "    wobble_distance=parse_number_value(args[\"wobble_distance\"], i)\n",
        "\n",
        "    wandImage=wandImage\n",
        "\n",
        "\n",
        "    something_changed = False\n",
        "\n",
        "    changes = \"\"\n",
        "    width = wandImage.width\n",
        "    height = wandImage.height\n",
        "    \n",
        "    if not wobble_distance == 0 and not wobble_angle == 0:\n",
        "      current_angle = (wobble_angle*i) % 360\n",
        "      current_angle_rad = math.radians(current_angle)\n",
        "      changes += f\"total angle {current_angle}\\n\"\n",
        "      changes += f\"total angle rad {current_angle_rad}\\n\"\n",
        "      x_vector = wobble_distance*math.cos(current_angle_rad)\n",
        "      y_vector = wobble_distance*math.sin(current_angle_rad)\n",
        "      changes += f\"x_vector {x_vector}\\n\"\n",
        "      changes += f\"y_vector {y_vector}\\n\"\n",
        "\n",
        "      translate_x += x_vector\n",
        "      translate_y += y_vector\n",
        "\n",
        "    if shake:\n",
        "      changes += f\"SHAKE!\"\n",
        "      wandImage.virtual_pixel = 'dither' # https://legacy.imagemagick.org/Usage/misc/#virtual-pixel\n",
        "      translate_x += float(random.randrange(0, 200)-100)/2000\n",
        "      translate_y += float(random.randrange(0, 200)-100)/2000\n",
        "      random_scale = float(random.randrange(0, 200)-100)/1000\n",
        "      scale_x += random_scale\n",
        "      scale_y += random_scale\n",
        "      angle += float(random.randrange(0, 200)-100)/10\n",
        "\n",
        "    # barrel\n",
        "    if not barrel_distort == 0 or not barrel_zoom == 1:\n",
        "      changes += f\"barrel_distort {barrel_distort}\\n\"\n",
        "      changes += f\"barrel_zoom {barrel_zoom}\\n\"\n",
        "      something_changed = True\n",
        "      barrel_args = (\n",
        "        barrel_distort,\n",
        "        0.0,\n",
        "        0.0,\n",
        "        barrel_zoom, # we do zoom with the scale too\n",
        "        width * barrel_origin_x, #X\n",
        "        height * barrel_origin_y, #Y\n",
        "        )\n",
        "      wandImage.distort('barrel', barrel_args)\n",
        "\n",
        "    # FLIP-FLOP\n",
        "    if flip and flop:\n",
        "        something_changed = True\n",
        "        # alternate flip/flop\n",
        "        if i % 2 == 0:\n",
        "            changes += \"flip\\n\"\n",
        "            wandImage.flip()\n",
        "        else:\n",
        "            changes += \"flop\\n\"\n",
        "            wandImage.flop()\n",
        "    elif flip:\n",
        "      changes += \"just flip\\n\"\n",
        "      something_changed = True\n",
        "      wandImage.flip()\n",
        "    elif flop:\n",
        "      changes += \"just flop\\n\"\n",
        "      something_changed = True\n",
        "      wandImage.flop()\n",
        "\n",
        "    if kaleidoscope:\n",
        "      wandImage.virtual_pixel = 'mirror' # https://legacy.imagemagick.org/Usage/misc/#virtual-pixel\n",
        "\n",
        "      quadrant = i % 4\n",
        "\n",
        "      if quadrant == 1:\n",
        "        changes += \"kaleidoscope - flip\\n\"\n",
        "        wandImage.flip()\n",
        "      elif quadrant == 2:\n",
        "        changes += \"kaleidoscope - flop\\n\"\n",
        "        wandImage.flop()\n",
        "      elif quadrant == 3:\n",
        "        changes += \"kaleidoscope - flip flop\\n\"\n",
        "        wandImage.flip()\n",
        "        wandImage.flop()\n",
        "      else:\n",
        "        changes += \"kaleidoscope\\n\"\n",
        "\n",
        "      something_changed = True\n",
        "      scale_rotate_args = (\n",
        "        0,\n",
        "        0,\n",
        "        1,\n",
        "        1,\n",
        "        0,\n",
        "        width/2,\n",
        "        height/2,\n",
        "      )\n",
        "      wandImage.distort('scale_rotate_translate', scale_rotate_args)\n",
        "\n",
        "      scale_rotate_args = (\n",
        "        0,\n",
        "        0,\n",
        "        1,\n",
        "        1,\n",
        "        0,\n",
        "        -width/2,\n",
        "        -height/2,\n",
        "      )\n",
        "      wandImage.distort('scale_rotate_translate', scale_rotate_args)\n",
        "\n",
        "    # scale rotate\n",
        "    if not scale_x == 1 or not scale_y == 1 or not angle == 0 or not translate_x == 0 or not translate_y == 0:\n",
        "      changes += f\"srt: scale_x {scale_x*100}% ({(width*scale_x)-width}px)\\n\"\n",
        "      changes += f\"srt: scale_y {scale_y*100}% ({(height*scale_y)-height}px)\\n\"\n",
        "      changes += f\"srt: angle {angle}\\n\"\n",
        "      changes += f\"srt: translate_x {translate_x*100}% ({(translate_x*width)}px)\\n\"\n",
        "      changes += f\"srt: translate_y {translate_y*100}% ({(translate_y*height)}px)\\n\"\n",
        "      changes += f\"srt: origin_x {origin_x*100}%\\n\"\n",
        "      changes += f\"srt: origin_y {origin_y*100}%\\n\"\n",
        "      something_changed = True\n",
        "      scale_rotate_args = (\n",
        "        width*origin_x,\n",
        "        height*origin_y,\n",
        "        scale_x, #ScaleX\n",
        "        scale_y, #ScaleY\n",
        "        angle, #Angle\n",
        "        (width*origin_x)+(translate_x*-width),\n",
        "        (height*origin_y)+(translate_y*height),\n",
        "      )\n",
        "\n",
        "      wandImage.virtual_pixel = 'black' # https://legacy.imagemagick.org/Usage/misc/#virtual-pixel\n",
        "      wandImage.distort('scale_rotate_translate', scale_rotate_args)\n",
        "\n",
        "    # Text\n",
        "    #draw = Drawing()\n",
        "    #draw.font_size = 50\n",
        "    #draw.text_under_color = \"#ffffff\"\n",
        "    #draw.text(10, 10, \"A\")\n",
        "    #draw(wandImage)\n",
        "    \n",
        "    #TODO: https://legacy.imagemagick.org/Usage/mapping/#distort\n",
        "    #TODO: http://www.fmwconcepts.com/imagemagick/index.php\n",
        "    \n",
        "    if swirl != 0:\n",
        "      changes += f\"swirl {swirl}\\n\"\n",
        "      something_changed = True\n",
        "      wandImage.swirl(swirl)\n",
        "\n",
        "    if fred_script != \"\":\n",
        "      something_changed = True\n",
        "      wandImage = run_script_on_image(i, wandImage, fred_script)\n",
        "\n",
        "    # Blur\n",
        "    if blur > 0:\n",
        "      changes += f\"blur {blur}\\n\"\n",
        "      something_changed = True\n",
        "      wandImage.blur(radius=blur, sigma=blur)\n",
        "\n",
        "    # Overlay Image\n",
        "    if overlay_image_filename != \"\":\n",
        "      changes += f\"overlay_image_filename {overlay_image_filename}\\n\"\n",
        "      something_changed = True\n",
        "\n",
        "      overlay = WandImage(filename = overlay_image_filename)\n",
        "\n",
        "      overay_angle = (i*0.25)*-1\n",
        "\n",
        "      scale_rotate_args = (\n",
        "        overlay.width*0.5,\n",
        "        overlay.height*0.5,\n",
        "        1, #ScaleX\n",
        "        1, #ScaleY\n",
        "        overay_angle, #Angle\n",
        "      )\n",
        "\n",
        "      overlay.virtual_pixel = 'mirror'\n",
        "      # overlay.distort('scale_rotate_translate', scale_rotate_args)\n",
        "\n",
        "      overlay.resize(width=int(wandImage.width*1),height=int(wandImage.height*1))\n",
        "\n",
        "      if overlay_operator == \"normal\":\n",
        "        changes += f\"normal\\n\"\n",
        "        wandImage.composite(overlay, left=int((wandImage.width-overlay.width)/2), top=int((wandImage.height-overlay.height)/2))\n",
        "      else:\n",
        "        changes += f\"overlay_operator: {overlay_operator}\\n\"\n",
        "        wandImage.composite(operator = overlay_operator, left = 0, top = 0, image = overlay)\n",
        "        # https://docs.wand-py.org/en/0.6.7/guide/draw.html#composite\n",
        "        # https://legacy.imagemagick.org/Usage/compose\n",
        "\n",
        "    # Blur Mask\n",
        "    if blur_mask_filename != \"\":\n",
        "      changes += f\"blur_mask_filename {blur_mask_filename}\\n\"\n",
        "      something_changed = True\n",
        "\n",
        "      overlay = WandImage(filename = blur_mask_filename)\n",
        "      wandImage.composite_channel('default_channels', overlay, 'blur', left = 0, top = 0, arguments = \"10\" )\n",
        "  \n",
        "    if not something_changed:\n",
        "      return None\n",
        "\n",
        "    show_image(wandImage,changes)\n",
        "    return wandImage\n",
        "\n",
        "def run_script_on_image(i, wand_image, script):\n",
        "    scratch_in = \"/content/gdrive/MyDrive/nat_gan/fred/scratch_in.png\"\n",
        "    scratch_out = \"/content/gdrive/MyDrive/nat_gan/fred/scratch_out.png\"\n",
        "    \n",
        "    wand_image.save(filename=scratch_in)\n",
        "    make_system_command(f\"bash {script} {scratch_in} {scratch_out}\")\n",
        "    return WandImage(filename = scratch_out)"
      ],
      "metadata": {
        "id": "5VsylcN5Em8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <h1>CLIP</h1> { display-mode: \"form\" }\n",
        "\n",
        "def build_clip_args():\n",
        "  # @markdown <p>VQGAN and Diffusion methods use this</p>\n",
        "  clip_model= \"ViT-B/32\" # @param [\"ViT-B/16\",\"ViT-B/32\"]\n",
        "  cut_pow=0.5 # @param {type:\"number\"}\n",
        "\n",
        "  # @markdown <br/><h4>Controls how many crops to take from the image. Increase for higher quality.</h4>\n",
        "  cutn= 64 # @param {type:\"number\"}\n",
        "\n",
        "  clip_args = {\n",
        "      \"cutn\" : cutn,\n",
        "      \"cut_pow\" : cut_pow,\n",
        "      \"clip_model\" : clip_model,\n",
        "    }\n",
        "\n",
        "  return clip_args\n",
        "\n",
        "def download_CLIP(device, working_dir, clip_model):\n",
        "  import sys\n",
        "  \n",
        "  if not file_exists(\"CLIP_libraries_installed\"):\n",
        "    print_step(\"Cloning CLIP...\")\n",
        "    make_system_command(f\"git clone https://github.com/openai/CLIP {working_dir}/downloads/CLIP\")\n",
        "    make_system_command(\"sudo pip install ftfy\")\n",
        "\n",
        "    make_system_command(\"sudo touch CLIP_libraries_installed\")\n",
        "\n",
        "\n",
        "  sys.path.append(f\"{working_dir}/downloads/CLIP\")\n",
        "  import clip\n",
        "  return clip.load(clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        cut_size,\n",
        "        cutn,\n",
        "        cut_pow\n",
        "        ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = nn.Sequential() #not in diffusion code\n",
        "        self.noise_fac = 0.1 #not in diffusion code\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        from torch.nn import functional as F\n",
        "\n",
        "        (sideY, sideX) = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "\n",
        "        if True: #diffuse stuff\n",
        "          for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "          return torch.cat(cutouts)\n",
        "        else: #old stuff\n",
        "          for _ in range(self.cutn):\n",
        "            size = int(torch.rand([]) ** self.cut_pow * (max_size\n",
        "                       - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:\n",
        "                           offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size,\n",
        "                           self.cut_size)))\n",
        "          batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "          if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0,\n",
        "                    self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "          return batch"
      ],
      "metadata": {
        "id": "UiCFCEgFCwjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title <h1>VQGAN+CLIP</h1> { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "def build_vqgan_args(model_dir):\n",
        "  yaml_path, ckpt_path, model_name = get_model(model_dir)\n",
        "\n",
        "  iterations_per_frame = \"15\"  # @param {type:\"string\"}\n",
        "  step_size = \"0.1\" # @param {type:\"string\"}\n",
        "  init_weight = 0. # @param {type:\"number\"}\n",
        "  look_back = False # @param {type:\"boolean\"}\n",
        "  stop_early = False # @param {type:\"boolean\"}\n",
        "\n",
        "  vqgan_args = {\n",
        "      \"look_back\" : look_back,\n",
        "      \"step_size\" : step_size,\n",
        "      \"init_weight\" : init_weight,\n",
        "      \"stop_early\" : stop_early,\n",
        "      \"iterations_per_frame\" : iterations_per_frame,\n",
        "      \"vqgan_checkpoint\" : ckpt_path,\n",
        "      \"vqgan_config\" : yaml_path,\n",
        "      \"model_name\": model_name,\n",
        "  }\n",
        "\n",
        "  return vqgan_args\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path, working_dir):\n",
        "    import sys\n",
        "    download_VQGAN(working_dir)\n",
        "\n",
        "    sys.path.append(f\"{working_dir}/downloads/taming-transformers\")\n",
        "\n",
        "    from omegaconf import OmegaConf\n",
        "    from taming.models import cond_transformer, vqgan\n",
        "\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == \"taming.models.vqgan.VQModel\":\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == \"taming.models.cond_transformer.Net2NetTransformer\":\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f\"unknown model type: {config.model.target}\")\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def download_VQGAN(working_dir):\n",
        "  import sys\n",
        "\n",
        "  if not file_exists(\"VQGAN_libraries_installed\"):\n",
        "\n",
        "    print_step(\"Cloning taming-transformers...\")\n",
        "    make_system_command(f\"git clone https://github.com/CompVis/taming-transformers {working_dir}/downloads/taming-transformers\")\n",
        "    \n",
        "    sys.path.append(f\"{working_dir}/downloads/taming-transformers\")\n",
        "\n",
        "    make_system_command(\"sudo pip install omegaconf pytorch-lightning einops\")\n",
        "    make_system_command(\"sudo touch VQGAN_libraries_installed\")\n",
        "\n",
        "\n",
        "def get_model(model_dir):\n",
        "  import os\n",
        "  model = \"ImageNet 16384\" #@param [\"ImageNet 16384\", \"ImageNet 1024\", \"WikiArt 1024\", \"WikiArt 16384\", \"COCO-Stuff\", \"FacesHQ\", \"S-FLCKR\", \"Gumbel 8192\", \"Ade20k\", \"FFHQ\", \"CelebA-HQ\"]\n",
        "\n",
        "  model_names={\n",
        "      \"ImageNet 16384\":{\n",
        "          \"ymal_url\":\"https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1\",\n",
        "          \"ckpt_url\":\"https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1\",\n",
        "          \"name\":\"vqgan_imagenet_f16_16384\"},\n",
        "      \"ImageNet 1024\":{\n",
        "          \"ymal_url\":\"https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1\",\n",
        "          \"ckpt_url\":\"https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1\",\n",
        "          \"name\":\"vqgan_imagenet_f16_1024\"}, \n",
        "      \"WikiArt 1024\":{\n",
        "          \"ymal_url\":\"http://mirror.io.community/blob/vqgan/wikiart.yaml\", #????\n",
        "          \"ckpt_url\":\"http://mirror.io.community/blob/vqgan/wikiart.ckpt\", #????\n",
        "          \"name\":\"wikiart_1024\"},\n",
        "      \"WikiArt 16384\":{\n",
        "          \"ymal_url\":\"http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml\",\n",
        "          \"ckpt_url\":\"http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt\",\n",
        "          \"name\":\"wikiart_16384\"},\n",
        "      \"COCO-Stuff\":{\n",
        "          \"ymal_url\":\"https://dl.nmkd.de/ai/clip/coco/coco.yaml\",\n",
        "          \"ckpt_url\":\"https://dl.nmkd.de/ai/clip/coco/coco.ckpt\",\n",
        "          \"name\":\"coco\"},\n",
        "      \"FacesHQ\":{\n",
        "          \"ymal_url\":\"https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT\",\n",
        "          \"ckpt_url\":\"https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt\",\n",
        "          \"name\":\"faceshq\"}, \n",
        "      \"S-FLCKR\":{\n",
        "          \"ymal_url\":\"https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1\",\n",
        "          \"ckpt_url\":\"https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1\",\n",
        "          \"name\":\"sflckr\"}, \n",
        "      \"Gumbel 8192\":{ # OpenImages 8912 ????\n",
        "          \"ymal_url\":\"https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1\",\n",
        "          \"ckpt_url\":\"https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fckpts%2Flast.ckpt&dl=1\",\n",
        "          \"name\":\"Gumbel_8192\"},\n",
        "      \"Ade20k\":{\n",
        "          \"ymal_url\":\"https://static.miraheze.org/intercriaturaswiki/b/bf/Ade20k.txt\",\n",
        "          \"ckpt_url\":\"https://app.koofr.net/content/links/0f65c2cd-7102-4550-a2bd-07fd383aac9e/files/get/last.ckpt?path=%2F2020-11-20T21-45-44_ade20k_transformer%2Fcheckpoints%2Flast.ckpt\",\n",
        "          \"name\":\"ADE20K\"},\n",
        "      \"FFHQ\":{\n",
        "          \"ymal_url\":\"https://app.koofr.net/content/links/0fc005bf-3dca-4079-9d40-cdf38d42cd7a/files/get/2021-04-23T18-19-01-project.yaml?path=%2F2021-04-23T18-19-01_ffhq_transformer%2Fconfigs%2F2021-04-23T18-19-01-project.yaml&force\",\n",
        "          \"ckpt_url\":\"https://app.koofr.net/content/links/0fc005bf-3dca-4079-9d40-cdf38d42cd7a/files/get/last.ckpt?path=%2F2021-04-23T18-19-01_ffhq_transformer%2Fcheckpoints%2Flast.ckpt&force\",\n",
        "          \"name\":\"FFHQ\"},\n",
        "      \"CelebA-HQ\":{\n",
        "          \"ymal_url\":\"https://app.koofr.net/content/links/6dddf083-40c8-470a-9360-a9dab2a94e96/files/get/2021-04-23T18-11-19-project.yaml?path=%2F2021-04-23T18-11-19_celebahq_transformer%2Fconfigs%2F2021-04-23T18-11-19-project.yaml&force\",\n",
        "          \"ckpt_url\":\"https://app.koofr.net/content/links/6dddf083-40c8-470a-9360-a9dab2a94e96/files/get/last.ckpt?path=%2F2021-04-23T18-11-19_celebahq_transformer%2Fcheckpoints%2Flast.ckpt&force\",\n",
        "          \"name\":\"CelebA-HQ\"}\n",
        "      }\n",
        "\n",
        "  if not os.path.isdir(f\"{model_dir}\"):\n",
        "      make_system_command(f\"sudo mkdir -p {model_dir}\")\n",
        "      print(\"model directory created\")\n",
        "\n",
        "  model_dict = model_names.get(model)\n",
        "  model_name = model_dict.get(\"name\")\n",
        "  model_ymal_url = model_dict.get(\"ymal_url\")\n",
        "  model_ckpt_url = model_dict.get(\"ckpt_url\")\n",
        "\n",
        "  print(f\"*** {model} ***\")\n",
        "\n",
        "  ckpt_path = f\"{model_dir}/{model_name}.ckpt\"\n",
        "  if not file_exists(ckpt_path):\n",
        "      print(f\"downloading {model} checkpoint from {model_ckpt_url}\")\n",
        "      # TODO: use urllib2 instead of curl\n",
        "      make_system_command(f\"sudo curl -L -o {ckpt_path} -C - '{model_ckpt_url}'\")\n",
        "  else:\n",
        "      print(f\"{model} checkpoint exists, skipping download\")\n",
        "\n",
        "  yaml_path = f\"{model_dir}/{model_name}.yaml\"\n",
        "  if not file_exists(yaml_path):\n",
        "      print(f\"downloading {model} model from {model_ymal_url}\")\n",
        "      # TODO: use urllib2 instead of wget\n",
        "      make_system_command(f\"sudo curl -L -o {yaml_path} -C - '{model_ymal_url}'\")\n",
        "  else:\n",
        "      print(f\"{model} model exists, skipping download\")\n",
        "    \n",
        "  return yaml_path, ckpt_path, model_name\n",
        "\n",
        "def run_vq_step(i, prompts, run, args, clip_model, cutouts, normalize, start_image, iterations, working_dir, device):\n",
        "    import queue, numpy, clip\n",
        "    from torch import optim\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    vqgan_model = load_vqgan_model(args[\"vqgan_config\"], args[\"vqgan_checkpoint\"],working_dir).to(device)\n",
        "    \n",
        "    start_image = make_wand_image(start_image)\n",
        "\n",
        "    #start_image.resize(720, 720)\n",
        "    start_image.resize(736, 736)\n",
        "\n",
        "    title = f\"\\n\\nVQGAN start {i}\\n\"\n",
        "\n",
        "    comp_distance = 25\n",
        "    recents = queue.Queue()\n",
        "    recents.maxsize = 10\n",
        "\n",
        "    last_i_count = 0\n",
        "\n",
        "    # look_back = args['look_back'] and iterations_per_frame > recents.maxsize/2\n",
        "    look_back = args[\"look_back\"] and iterations >= 5\n",
        "\n",
        "    z = z_from_img(start_image, vqgan_model, device)\n",
        "\n",
        "    if look_back:\n",
        "        title += f\"smooth count: {smooth_count}\\n\"\n",
        "\n",
        "    seed_image = wand_image_from_z(z, vqgan_model)\n",
        "    z.requires_grad_(True)\n",
        "    step_size = parse_number_value(args[\"step_size\"], i)\n",
        "    opt = optim.Adam([z], lr=step_size)\n",
        "    title += f\"step size: {step_size}\\n\"\n",
        "\n",
        "    pMs = []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        txt, weight, stop = parse_prompt(prompt)\n",
        "        embed = clip_model.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "        pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "        title += f\"prompt: {txt}\\nweight: {weight}\\n\"\n",
        "\n",
        "    print(title)\n",
        "\n",
        "    max_comp = 0\n",
        "    last_max = 0\n",
        "    prev_i = numpy.array(wand_image_from_z(z, vqgan_model))\n",
        "\n",
        "    if look_back:\n",
        "        iterations += recents.maxsize\n",
        "\n",
        "    for j in tqdm(range(iterations)):\n",
        "        last_i_count = j\n",
        "        #torch.cuda.empty_cache()\n",
        "        title = f\"\\\n",
        "        {' | '.join(prompts)}\\n\\\n",
        "        {run['run_name']} ({i}) ipf {iterations}\\n\\\n",
        "        last_i_count: {last_i_count}\"\n",
        "\n",
        "        train(i, opt, z, normalize, pMs, args['init_weight'], title, cutouts, vqgan_model, clip_model)\n",
        "\n",
        "        if look_back:\n",
        "            if recents.full():\n",
        "                recents.get()\n",
        "            recents.put(wand_image_from_z(z))\n",
        "\n",
        "        if not args['stop_early']:\n",
        "            continue\n",
        "\n",
        "        if j == 0:\n",
        "            prev_comp_image = numpy.array(wand_image_from_z(z))\n",
        "        if j % comp_distance == 0 and j >= comp_distance:\n",
        "            comp_image = numpy.array(wand_image_from_z(z))\n",
        "\n",
        "            comp = comp_images(prev_comp_image, comp_image)\n",
        "            prev_comp_image = comp_image\n",
        "            new_max = max(max_comp, comp)\n",
        "            if new_max is not max_comp:\n",
        "                last_max = j\n",
        "                print(f\"max*({j}): {round(comp.real, 5)}\")\n",
        "                max_comp = new_max\n",
        "            else:\n",
        "                print(f\"nope({j}): {round(comp.real, 5)}\")\n",
        "            if (j - last_max) >= (comp_distance * 2):\n",
        "                break\n",
        "\n",
        "    if look_back:\n",
        "        out = synth(z)\n",
        "        img = img_from_z(out)\n",
        "\n",
        "        best_image = recents.get()\n",
        "\n",
        "        seed_image_array = numpy.array(seed_image)\n",
        "\n",
        "        comp = comp_images(seed_image_array, numpy.array(best_image))\n",
        "        max_comp = comp\n",
        "        print(\"start\")\n",
        "\n",
        "        no_count = 0\n",
        "        for prev_image in recents.queue:\n",
        "            comp = comp_images(seed_image_array, numpy.array(prev_image))\n",
        "            if comp > max_comp:\n",
        "                max_comp = comp\n",
        "                best_image = prev_image\n",
        "                print(\"yes\")\n",
        "                smooth_count += 1\n",
        "                no_count = 0\n",
        "            else:\n",
        "                no_count += 1\n",
        "                print(\"no\")\n",
        "                # if no_count > 3:\n",
        "                # break\n",
        "\n",
        "    out = synth(z, vqgan_model)\n",
        "    img = img_from_z(out)\n",
        "    show_image(img, \"finished vqgan\", scale=0.7)\n",
        "    return img\n",
        "\n",
        "def comp_images(a, b):\n",
        "    # https://github.com/andrewekhalel/sewar\n",
        "    comp = msssim(a, b)\n",
        "    return comp\n",
        "\n",
        "def img_from_z(synthed):\n",
        "  import numpy\n",
        "  z_array = (\n",
        "      synthed\n",
        "      .mul(255)\n",
        "      .clamp(0, 255)\n",
        "      [0]\n",
        "      .cpu()\n",
        "      .detach()\n",
        "      .numpy()\n",
        "      .astype(numpy.uint8)\n",
        "      )\n",
        "  one = numpy.array(z_array)[:, :, :]\n",
        "  img = numpy.transpose(z_array, (1, 2, 0))\n",
        "  return img\n",
        "\n",
        "def z_from_img(img, vqgan_model, device):\n",
        "  from torchvision.transforms import functional as TF\n",
        "  import numpy\n",
        "\n",
        "  four = numpy.array(img)\n",
        "  three = TF.to_tensor(four)\n",
        "  two = three.to(device)\n",
        "  one = two.unsqueeze(0) * 2 - 1\n",
        "  z, *_ = vqgan_model.encode(one)\n",
        "  return z\n",
        "\n",
        "def wand_image_from_z(z, vqgan_model):\n",
        "    import numpy\n",
        "    from wand.image import Image as WandImage\n",
        "\n",
        "    z_array = (\n",
        "      synth(z, vqgan_model).movedim(3, 2)\n",
        "      .mul(255)\n",
        "      .clamp(0, 255)\n",
        "      [0]\n",
        "      .cpu()\n",
        "      .detach()\n",
        "      .numpy()\n",
        "      .astype(numpy.uint8)\n",
        "    )\n",
        "\n",
        "    numpy_array = numpy.asarray(z_array)\n",
        "    width = numpy_array.shape[1]\n",
        "    height = numpy_array.shape[2]\n",
        "    reshaped = numpy.reshape(numpy_array,(width, height, 3), order='F')\n",
        "    return WandImage.from_array(reshaped)\n",
        "\n",
        "\n",
        "def synth(z, vqgan_model):\n",
        "  movedim = z.movedim(1, 3)\n",
        "\n",
        "  weight = vqgan_model.quantize.embedding.weight\n",
        "\n",
        "  z_q = vector_quantize(movedim, weight).movedim(3, 1)\n",
        "\n",
        "  grad = vqgan_model.decode(z_q).add(1).div(2)\n",
        "\n",
        "  clamp_with_grad = ClampWithGrad.apply\n",
        "  clamp = clamp_with_grad(grad, 0, 1)\n",
        "  return clamp\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    from torch.nn import functional as F\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) \\\n",
        "        - 2 * x  @codebook.T\n",
        "\n",
        "    indices = d.argmin(-1)\n",
        "\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype)  @codebook\n",
        "\n",
        "    replace_grad = ReplaceGrad.apply\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return (None, grad_in.sum_to_size(ctx.shape))\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(\n",
        "        ctx,\n",
        "        input,\n",
        "        min,\n",
        "        max,\n",
        "        ):\n",
        "\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        (input, ) = ctx.saved_tensors\n",
        "        return (grad_in * (grad_in * (input - input.clamp(ctx.min,\n",
        "                ctx.max)) >= 0), None, None)\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed,\n",
        "        weight=1.,\n",
        "        stop=float('-inf'),\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        from torch.nn import functional as F\n",
        "\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0),\n",
        "                dim=2)\n",
        "        dists = \\\n",
        "            input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "\n",
        "        replace_grad = ReplaceGrad.apply\n",
        "        return self.weight.abs() * replace_grad(dists,\n",
        "                torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "def train(i, opt, z, normalize, pMs, init_weight, title, cutouts, vqgan_model, clip_model):\n",
        "    opt.zero_grad()\n",
        "    losses = ascend_txt(i, z, normalize, pMs, init_weight, cutouts, vqgan_model, clip_model)\n",
        "    loss = sum(losses)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    z_min = vqgan_model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = vqgan_model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "def ascend_txt(i, z, normalize, pMs, init_weight, cutouts, vqgan_model, clip_model):\n",
        "    out = synth(z, vqgan_model)\n",
        "    iii = clip_model.encode_image(normalize(cutouts(out))).float()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if init_weight:\n",
        "        result.append(F.mse_loss(z, z_orig) * init_weight / 2)\n",
        "\n",
        "    for prompt in pMs:\n",
        "        result.append(prompt(iii))\n",
        "\n",
        "    return result\n",
        "    "
      ],
      "metadata": {
        "id": "dfMrFSp__KZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <h1>CLIP Guided Diffusion</h1> { display-mode: \"form\" }\n",
        "\n",
        "exec(open(f'{working_dir}/downloads/modules/clip-diffusion.py').read())\n",
        "\n",
        "def build_diffusion_args(model_dir):\n",
        "  d_checkpoint = \"512x512_diffusion_uncond_finetune_008100\" #@param [\"512x512_diffusion\", \"512x512_diffusion_uncond_finetune_008100\"]\n",
        "\n",
        "  # https://github.com/afiaka87/clip-guided-diffusion\n",
        "  # https://github.com/openai/guided-diffusion\n",
        "  # https://github.com/crowsonkb/v-diffusion-pytorch\n",
        "  # https://thisnameismine.com/programming/360diffusion-real-esrgan-integrated-clip-guided-diffusion/\n",
        "  # https://github.com/crowsonkb/guided-diffusion\n",
        "\n",
        "  diff_use_init = False # @param {type:\"boolean\"}\n",
        "\n",
        "  # @markdown <br/><h4>Controls the starting point along the diffusion timesteps.\n",
        "  # @markdown Higher values make the output look more like the init.</h4>\n",
        "  skip_timesteps_1 = \"{\\\"frames\\\": [0,1],\\\"values\\\": [0,400]}\" # @param {type:\"string\"}\n",
        "\n",
        "  skip_timesteps_2 = \"0\" # @param {type:\"string\"}\n",
        "\n",
        "  mid_run_save =  0# @param {type:\"number\"}\n",
        "  mid_run_show = 10 # @param {type:\"number\"}\n",
        "\n",
        "  # @markdown <br/><h4>the batch size (default: 1)</h4>\n",
        "  batch_size = 1 # @param {type:\"number\"}\n",
        "\n",
        "  # @markdown <br/><h4>Diffusion steps (default: 1000)</h4>\n",
        "  diffusion_steps = 1000 # @param {type:\"number\"}\n",
        "\n",
        "  # @markdown <br/><h4>Timestep respacing (default: 1000)</h4>\n",
        "  timestep_respacing = \"1000\" # @param {type:\"string\"}\n",
        "\n",
        "  # @markdown <br/><h4>Scale for CLIP spherical distance loss. Values will need tinkering for different settings. (default: 1000)</h4>\n",
        "  clip_guidance_scale = 1000  # @param {type:\"number\"} \n",
        "\n",
        "  # @markdown <br/><h4>(optional) Perceptual loss scale for init image. Good at 1000?(default: 0)</h4>\n",
        "  init_scale =  0# @param {type:\"number\"}\n",
        "\n",
        "  # @markdown <br/><h4>Controls the smoothness of the final output. (default:150.0)</h4>\n",
        "  tv_scale =  150 # @param {type:\"number\"}\n",
        "\n",
        "  # @markdown <br/><h4>range_scaleControls how far out of RGB range values may get.(default: 50.0)</h4>\n",
        "  range_scale =  50# @param {type:\"number\"}\n",
        "\n",
        "  # @markdown <br/><h4>Specify noise schedule. Either 'linear' or 'cosine'.(default: linear)</h4>\n",
        "  noise_schedule = \"linear\"  #@param [\"linear\",\"cosine\"]\n",
        "\n",
        "  # @markdown <br/><h4>Accumulate CLIP gradient from multiple batches of cuts. Can help with OOM errors / Low VRAM. (default: 2)</h4>\n",
        "  cutn_batches = 1 # @param {type:\"number\"}\n",
        "\n",
        "  # @markdown <br/><h4>n_batches ?? </h4>\n",
        "  n_batches = 1 # @param {type:\"number\"}\n",
        "\n",
        "  diffusion_args = { \n",
        "      \"d_checkpoint_url_path\" : get_checkpoints(model_dir, d_checkpoint),\n",
        "      \"diff_use_init\" : diff_use_init,\n",
        "      \"n_batches\" : n_batches,\n",
        "      \"batch_size\" : batch_size,\n",
        "      \"cutn_batches\" : cutn_batches,\n",
        "      \"clip_guidance_scale\" : clip_guidance_scale,\n",
        "      \"tv_scale\" : tv_scale,\n",
        "      \"range_scale\" : range_scale,\n",
        "      \"init_scale\" : init_scale,\n",
        "      \"mid_run_save\" : mid_run_save,\n",
        "      \"mid_run_show\" : mid_run_show,\n",
        "      \"diffusion_steps\" : diffusion_steps,\n",
        "      \"timestep_respacing\" : timestep_respacing,\n",
        "      \"noise_schedule\" : noise_schedule,\n",
        "      \"skip_timesteps_1\" : skip_timesteps_1,\n",
        "      \"skip_timesteps_2\" : skip_timesteps_2,\n",
        "    }\n",
        "\n",
        "  return diffusion_args"
      ],
      "metadata": {
        "id": "zvftTgM0_Xd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin"
      ],
      "metadata": {
        "id": "wFUjjTFDX4aD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqvIPS0R0gWr"
      },
      "outputs": [],
      "source": [
        "#@title  <h1>Begin Sequence</h1>  { form-width: \"200px\", display-mode: \"form\" }\n",
        "def prepare_run(working_dir):\n",
        "  import json\n",
        "\n",
        "  download_common_stuff()\n",
        "\n",
        "  replace_yaml = True # @param {type:\"boolean\"}\n",
        "\n",
        "  model_dir = f\"{working_dir}/downloads/models\"\n",
        "\n",
        "  prompt_args = build_prompt_args()\n",
        "  run_args = build_run_args()\n",
        "  vqgan_args = build_vqgan_args(model_dir)\n",
        "\n",
        "  output_path = f'{working_dir}/output/{run_args[\"run_name\"]}'\n",
        "  make_system_command(f\"sudo mkdir --parents {output_path}\")\n",
        "  json_path = f'{output_path}/params.json'\n",
        "\n",
        "  manipulste_args = build_manipulste_args(output_path)\n",
        "  clip_args = build_clip_args()\n",
        "\n",
        "  diffusion_args = build_diffusion_args(model_dir)\n",
        "\n",
        "  user_args = {\n",
        "    \"prompt\" : prompt_args,\n",
        "    \"run\" : run_args,\n",
        "    \"CLIP\" : clip_args,\n",
        "    \"VQGAN\" : vqgan_args,\n",
        "    \"diffusion\" : diffusion_args,\n",
        "    \"manipulate\" : manipulste_args,\n",
        "  }\n",
        "\n",
        "  if replace_yaml or not file_exists(json_path):\n",
        "    with open(json_path, 'w') as fp:\n",
        "      json.dump(user_args, fp, sort_keys=True, indent=4)\n",
        "  else:\n",
        "    with open(json_path) as json_file:\n",
        "      user_args = json.load(json_file)\n",
        "    \n",
        "  return output_path, user_args\n",
        "\n",
        "def begin_art(working_dir):\n",
        "  from IPython.display import clear_output\n",
        "  import random, gc, json, os\n",
        "\n",
        "  # Delete memory from previous runs\n",
        "  make_system_command(f\"nvidia-smi -caa\")\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  print('Using device:', device)\n",
        "\n",
        "  !nvidia-smi\n",
        "\n",
        "  output_path, user_args = prepare_run(working_dir)\n",
        "\n",
        "  last_i_count = 0\n",
        "  itteration_count = 0\n",
        "  frame_count = 0\n",
        "  smooth_count = 0\n",
        "  make_system_command(f\"nvidia-smi -caa\")\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  stop_on_next_loop = False\n",
        "  while i <= user_args[\"run\"][\"end_frame\"]:\n",
        "    if stop_on_next_loop:\n",
        "        break\n",
        "    try:\n",
        "        with open(f'{output_path}/params.json') as json_file:\n",
        "            user_args = json.load(json_file)\n",
        "\n",
        "        run_name = user_args[\"run\"][\"run_name\"]\n",
        "\n",
        "        while file_exists(make_image_filename(i, working_dir, run_name)):\n",
        "          i = next_i(i, user_args[\"run\"][\"end_frame\"],f'{output_path}/',run_name)\n",
        "\n",
        "        if user_args[\"run\"][\"ramdom_seed\"]:\n",
        "          import random\n",
        "          random.seed(None)\n",
        "          user_args[\"run\"][\"seed\"] = random.randint(1, 99999999)\n",
        "\n",
        "        seed = user_args[\"run\"]['seed']\n",
        "        random.seed(seed)\n",
        "        import numpy\n",
        "        numpy.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "        print(f\"seed: {seed}\")\n",
        "\n",
        "\n",
        "        clip_model = download_CLIP(device, working_dir, user_args[\"CLIP\"][\"clip_model\"])\n",
        "\n",
        "        if \"cutouts\" not in locals():\n",
        "          cutouts = MakeCutouts(clip_model.visual.input_resolution, user_args[\"CLIP\"][\"cutn\"], user_args[\"CLIP\"][\"cut_pow\"])\n",
        "\n",
        "        from torchvision import transforms\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                          std=[0.26862954, 0.26130258, 0.27577711])\n",
        "          \n",
        "\n",
        "        prompts = get_prompts_for_i(i, user_args[\"prompt\"])\n",
        "\n",
        "\n",
        "        name_override = None\n",
        "        if user_args[\"prompt\"][\"add_random_adjectives\"] or user_args[\"prompt\"][\"add_random_noun\"] or user_args[\"prompt\"][\"add_random_styles\"]:\n",
        "            name_override = \"\"\n",
        "            for prompt in prompts:\n",
        "                txt, _, _ = parse_prompt(prompt)\n",
        "                name_override += f\"[{txt}]\"\n",
        "\n",
        "        name_override = None\n",
        "\n",
        "        latest_image = next_start_image(i, user_args[\"run\"][\"end_frame\"],run_name,working_dir, user_args[\"prompt\"])\n",
        "\n",
        "        iterations = parse_number_value(user_args[\"VQGAN\"][\"iterations_per_frame\"], i, integer=True)\n",
        "        \n",
        "        show_image(latest_image, f\"start step {i}\")\n",
        "        \n",
        "        if iterations > 4:\n",
        "            wandImage = make_wand_image(latest_image)\n",
        "            wandImage.alpha_channel = False\n",
        "            manipulated = manipulate_image(wandImage, user_args['manipulate'],i)\n",
        "\n",
        "            if manipulated is not None:\n",
        "                latest_image = manipulated\n",
        "\n",
        "        try:\n",
        "          save_image(i,latest_image,run_name,name_override=name_override,include_latest=False)\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        steps_taken = \"\"\n",
        "\n",
        "        if user_args[\"run\"][\"continuous_vqgan\"]:\n",
        "          print(\"continuous_vqgan\")\n",
        "          overlay_path = user_args['manipulate'][\"overlay_image_filename\"]\n",
        "          latest_edit_time = os.path.getmtime(overlay_path) + os.path.getmtime(f'{output_path}/params.json')\n",
        "          while(os.path.getmtime(overlay_path) + os.path.getmtime(f'{output_path}/params.json') == latest_edit_time):\n",
        "              latest_edit_time = os.path.getmtime(overlay_path) + os.path.getmtime(f'{output_path}/params.json')\n",
        "              latest_image = run_vq_step(i, prompts, user_args[\"run\"], user_args[\"VQGAN\"], cutouts, latest_image, iterations)\n",
        "              save_image(i,latest_image,run_name,name_override=name_override,include_latest=False)\n",
        "\n",
        "        for _ in range(user_args[\"run\"][\"vqgan_1_runs\"]):\n",
        "          steps_taken += \"[vqgan]\"\n",
        "          latest_image = run_vq_step(i, prompts, user_args[\"run\"], user_args[\"VQGAN\"], clip_model, cutouts, normalize, latest_image, iterations, working_dir, device)\n",
        "          if user_args[\"run\"][\"save_mid_steps\"]:\n",
        "            save_image(i,latest_image,run_name,subfolder=steps_taken,name_override=name_override,include_latest=False,suffix=steps_taken)\n",
        "\n",
        "        for _ in range(user_args[\"run\"][\"diff_1_runs\"]):\n",
        "          steps_taken += \"[diff]\"\n",
        "          skip_timesteps = parse_number_value(user_args[\"diffusion\"][\"skip_timesteps_1\"], i, integer=True)\n",
        "          latest_image = run_d_step(i, device, prompts, user_args[\"diffusion\"], user_args[\"prompt\"], user_args[\"CLIP\"], user_args[\"run\"], clip_model, cutouts, normalize, skip_timesteps, latest_image, steps_taken)\n",
        "          if user_args[\"run\"][\"save_mid_steps\"]:\n",
        "            save_image(i,latest_image,run_name,subfolder=steps_taken,name_override=name_override,include_latest=False,suffix=steps_taken)\n",
        "\n",
        "        for _ in range(user_args[\"run\"][\"vqgan_2_runs\"]):\n",
        "          steps_taken += \"[vqgan]\"\n",
        "          latest_image = run_vq_step(i, prompts, user_args[\"run\"], user_args[\"VQGAN\"], clip_model, cutouts, normalize, latest_image, iterations, working_dir, device)\n",
        "          if user_args[\"run\"][\"save_mid_steps\"]:\n",
        "            save_image(i,latest_image,run_name,subfolder=steps_taken,name_override=name_override,include_latest=False,suffix=steps_taken)\n",
        "\n",
        "        for _ in range(user_args[\"run\"][\"diff_2_runs\"]):\n",
        "          steps_taken += \"[diff]\"\n",
        "          skip_timesteps = parse_number_value(user_args[\"diffusion\"][\"skip_timesteps_2\"], i, integer=True)\n",
        "          latest_image = run_d_step(i, device,  prompts, user_args[\"diffusion\"], user_args[\"prompt\"], user_args[\"CLIP\"], user_args[\"run\"],  clip_model, cutouts, normalize, skip_timesteps, latest_image, steps_taken)\n",
        "          if user_args[\"run\"][\"save_mid_steps\"]:\n",
        "            save_image(i,latest_image,run_name,subfolder=steps_taken,name_override=name_override,include_latest=False,suffix=steps_taken)\n",
        "\n",
        "        for _ in range(user_args[\"run\"][\"vqgan_3_runs\"]):\n",
        "          steps_taken += \"[vqgan]\"\n",
        "          latest_image = run_vq_step(i, prompts, user_args[\"run\"], user_args[\"VQGAN\"], clip_model, cutouts, normalize, latest_image, iterations, working_dir, device)\n",
        "          if user_args[\"run\"][\"save_mid_steps\"]:\n",
        "            save_image(i,latest_image,run_name,subfolder=steps_taken,name_override=name_override,include_latest=False,suffix=steps_taken)\n",
        "\n",
        "        save_image(i, working_dir, latest_image,run_name, user_args, prompts ,name_override=name_override,include_latest=False)\n",
        "        clear_output(wait=True)\n",
        "        i = next_i(i, user_args[\"run\"][\"end_frame\"],f'{output_path}/',run_name)\n",
        "    except KeyboardInterrupt:\n",
        "        stop_on_next_loop = True\n",
        "        print(\"STOP REQUESTED!\")\n",
        "        pass\n",
        "  clear_output(wait=True)\n",
        "  show_image(latest_image, f\"DONE! ({i})\")\n",
        "\n",
        "exec(open(f'{working_dir}/downloads/modules/common.py').read())\n",
        "exec(open(f'{working_dir}/downloads/modules/prompting.py').read())\n",
        "\n",
        "begin_art(working_dir)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Nat-GAN 2022-03-08",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}